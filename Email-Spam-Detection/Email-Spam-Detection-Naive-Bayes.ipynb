{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produced by: Kirubel Temesgen\n",
    "# College ID: C00260396\n",
    "# Description: Machine Learning Algorithm Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"D:\\college\\Sem2\\DataSci ML\\archive\\emails.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display dataset information\n",
    "print(df.info())\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TESTING\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset (only 25% of rows)\n",
    "file_path = r\"D:\\college\\Sem2\\DataSci ML\\archive\\emails.csv\"\n",
    "df = pd.read_csv(file_path).sample(frac=0.1, random_state=42)  # Adjust frac=0.5 for 50%, 0.1 for 10%\n",
    "\n",
    "# Reset index after sampling\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Display dataset information\n",
    "print(df.info())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate messages\n",
    "duplicate_count = df.duplicated(subset=['message']).sum()\n",
    "print(f\"Number of duplicate emails: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to extract email components\n",
    "def extract_email_components(email):\n",
    "    from_match = re.search(r'From: (.+)', email)\n",
    "    to_match = re.search(r'To: (.+)', email)\n",
    "    subject_match = re.search(r'Subject: (.*)', email)\n",
    "    body_match = re.search(r'\\n\\n(.*)', email, re.DOTALL)  # Body starts after a blank line\n",
    "\n",
    "    return {\n",
    "        \"From\": from_match.group(1).strip() if from_match else None,\n",
    "        \"To\": to_match.group(1).strip() if to_match else None,\n",
    "        \"Subject\": subject_match.group(1).strip() if subject_match else None,\n",
    "        \"Body\": body_match.group(1).strip() if body_match else None\n",
    "    }\n",
    "\n",
    "# Apply function to all emails\n",
    "email_components = df['message'].apply(extract_email_components)\n",
    "email_df = pd.DataFrame(email_components.tolist())\n",
    "\n",
    "# Merge extracted data with original DataFrame\n",
    "df = pd.concat([df, email_df], axis=1)\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any spam labels\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in key fields\n",
    "print(\"Missing values per column:\")\n",
    "print(df[['From', 'To', 'Subject', 'Body']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check for duplicate messages\n",
    "duplicate_count = df.duplicated(subset=['message']).sum()\n",
    "print(f\"Number of duplicate emails: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most active email senders\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['From'].value_counts().head(20).plot(kind='bar', figsize=(10,5))\n",
    "plt.title(\"Top 20 Most Frequent Email Senders\")\n",
    "plt.xlabel(\"Email Address\")\n",
    "plt.ylabel(\"Number of Emails\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "word_counts = Counter(\" \".join(df['Subject'].fillna(\"\")).split())\n",
    "common_words = pd.DataFrame(word_counts.most_common(20), columns=[\"Word\", \"Frequency\"])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=\"Frequency\", y=\"Word\", data=common_words)\n",
    "plt.title(\"Most Common Words in Subject Lines\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b(re|fw)\\b', '', text)  # Remove \"Re:\" and \"FW:\"\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\d+', '', text)   # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to Subject & Body\n",
    "df[\"Processed_Subject\"] = df[\"Subject\"].fillna(\"\").apply(clean_text)\n",
    "df[\"Processed_Body\"] = df[\"Body\"].fillna(\"\").apply(clean_text)\n",
    "\n",
    "# Display cleaned text\n",
    "print(df[[\"Processed_Subject\", \"Processed_Body\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.dropna(subset=[\"From\", \"Processed_Subject\", \"Processed_Body\"])\n",
    "print(\"Remaining missing values:\\n\", df.isnull().sum())  # Verify no missing values remain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define spam keywords\n",
    "spam_keywords = [\n",
    "    \"win\", \"lottery\", \"free\", \"offer\", \"click here\", \"urgent\", \"claim\", \n",
    "    \"limited-time\", \"money\", \"prize\", \"limited time\"\n",
    "]\n",
    "\n",
    "# Compile regex pattern for keyword matching\n",
    "spam_pattern = re.compile(r'\\b(' + '|'.join(spam_keywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Function to check if an email is spam\n",
    "def is_spam(row):\n",
    "    subject = row[\"Processed_Subject\"] if pd.notnull(row[\"Processed_Subject\"]) else \"\"\n",
    "    body = row[\"Processed_Body\"] if pd.notnull(row[\"Processed_Body\"]) else \"\"\n",
    "    sender = row[\"From\"] if pd.notnull(row[\"From\"]) else \"\"\n",
    "\n",
    "    # Check for spam keywords in subject or body\n",
    "    keyword_match = bool(spam_pattern.search(subject)) or bool(spam_pattern.search(body))\n",
    "\n",
    "    # Check if sender is outside Enron (not ending in @enron.com)\n",
    "    domain_check = not sender.endswith(\"@enron.com\") if sender else False\n",
    "\n",
    "    return 1 if keyword_match or domain_check else 0\n",
    "\n",
    "# Apply function\n",
    "df[\"Spam_Label\"] = df.apply(is_spam, axis=1)\n",
    "\n",
    "# Display spam vs non-spam counts\n",
    "print(df[\"Spam_Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'To' column as it's not useful\n",
    "df = df.drop(columns=[\"To\"], errors=\"ignore\")\n",
    "\n",
    "# Remove rows with missing values in key fields\n",
    "df = df.dropna(subset=[\"From\", \"Processed_Subject\", \"Processed_Body\"])\n",
    "\n",
    "# Feature Engineering: Word Count in Subject & Body\n",
    "df[\"Word_Count_Subject\"] = df[\"Processed_Subject\"].apply(lambda x: len(x.split()))\n",
    "df[\"Word_Count_Body\"] = df[\"Processed_Body\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Display updated dataset\n",
    "print(df[[\"Processed_Subject\", \"Processed_Body\", \"Word_Count_Subject\", \"Word_Count_Body\", \"Spam_Label\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialise TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=500, ngram_range=(1,2))\n",
    "\n",
    "# Transform Subject & Body separately\n",
    "tfidf_subject = vectorizer.fit_transform(df[\"Processed_Subject\"])\n",
    "tfidf_body = vectorizer.fit_transform(df[\"Processed_Body\"])\n",
    "\n",
    "# Merge TF-IDF features into a single sparse matrix\n",
    "X = hstack([tfidf_subject, tfidf_body])  # Sparse format to reduce memory usage\n",
    "y = df[\"Spam_Label\"].values  # Convert labels to NumPy array\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))  # Should be <class 'scipy.sparse._csr.csr_matrix'>\n",
    "print(X_train.shape)  # Should match expected training size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"Spam_Label\", \"From\", \"file\", \"message\"], errors=\"ignore\")\n",
    "y = df[\"Spam_Label\"]\n",
    "\n",
    "# Split dataset (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"y_train type: {type(y_train)}\")\n",
    "print(f\"Unique values in y_train: {np.unique(y_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))  \n",
    "print(X_train.shape)  # Should match expected training size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Merge subject and body TF-IDF vectors\n",
    "X = hstack([tfidf_subject, tfidf_body])  # Ensure it's a sparse matrix\n",
    "y = df[\"Spam_Label\"].values  # Convert labels to NumPy array\n",
    "\n",
    "# Split Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"D:\\college\\Sem2\\DataSci ML\\archive\\emails.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display dataset information\n",
    "print(df.info())\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TESTING\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset (only 25% of rows)\n",
    "file_path = r\"D:\\college\\Sem2\\DataSci ML\\archive\\emails.csv\"\n",
    "df = pd.read_csv(file_path).sample(frac=0.1, random_state=42)  # Adjust frac=0.5 for 50%, 0.1 for 10%\n",
    "\n",
    "# Reset index after sampling\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Display dataset information\n",
    "print(df.info())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate messages\n",
    "duplicate_count = df.duplicated(subset=['message']).sum()\n",
    "print(f\"Number of duplicate emails: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to extract email components\n",
    "def extract_email_components(email):\n",
    "    from_match = re.search(r'From: (.+)', email)\n",
    "    to_match = re.search(r'To: (.+)', email)\n",
    "    subject_match = re.search(r'Subject: (.*)', email)\n",
    "    body_match = re.search(r'\\n\\n(.*)', email, re.DOTALL)  # Body starts after a blank line\n",
    "\n",
    "    return {\n",
    "        \"From\": from_match.group(1).strip() if from_match else None,\n",
    "        \"To\": to_match.group(1).strip() if to_match else None,\n",
    "        \"Subject\": subject_match.group(1).strip() if subject_match else None,\n",
    "        \"Body\": body_match.group(1).strip() if body_match else None\n",
    "    }\n",
    "\n",
    "# Apply function to all emails\n",
    "email_components = df['message'].apply(extract_email_components)\n",
    "email_df = pd.DataFrame(email_components.tolist())\n",
    "\n",
    "# Merge extracted data with original DataFrame\n",
    "df = pd.concat([df, email_df], axis=1)\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any spam labels\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in key fields\n",
    "print(\"Missing values per column:\")\n",
    "print(df[['From', 'To', 'Subject', 'Body']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check for duplicate messages\n",
    "duplicate_count = df.duplicated(subset=['message']).sum()\n",
    "print(f\"Number of duplicate emails: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most active email senders\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['From'].value_counts().head(20).plot(kind='bar', figsize=(10,5))\n",
    "plt.title(\"Top 20 Most Frequent Email Senders\")\n",
    "plt.xlabel(\"Email Address\")\n",
    "plt.ylabel(\"Number of Emails\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "word_counts = Counter(\" \".join(df['Subject'].fillna(\"\")).split())\n",
    "common_words = pd.DataFrame(word_counts.most_common(20), columns=[\"Word\", \"Frequency\"])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=\"Frequency\", y=\"Word\", data=common_words)\n",
    "plt.title(\"Most Common Words in Subject Lines\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b(re|fw)\\b', '', text)  # Remove \"Re:\" and \"FW:\"\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\d+', '', text)   # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to Subject & Body\n",
    "df[\"Processed_Subject\"] = df[\"Subject\"].fillna(\"\").apply(clean_text)\n",
    "df[\"Processed_Body\"] = df[\"Body\"].fillna(\"\").apply(clean_text)\n",
    "\n",
    "# Display cleaned text\n",
    "print(df[[\"Processed_Subject\", \"Processed_Body\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.dropna(subset=[\"From\", \"Processed_Subject\", \"Processed_Body\"])\n",
    "print(\"Remaining missing values:\\n\", df.isnull().sum())  # Verify no missing values remain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define spam keywords\n",
    "spam_keywords = [\n",
    "    \"win\", \"lottery\", \"free\", \"offer\", \"click here\", \"urgent\", \"claim\", \n",
    "    \"limited-time\", \"money\", \"prize\", \"limited time\"\n",
    "]\n",
    "\n",
    "# Compile regex pattern for keyword matching\n",
    "spam_pattern = re.compile(r'\\b(' + '|'.join(spam_keywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Function to check if an email is spam\n",
    "def is_spam(row):\n",
    "    subject = row[\"Processed_Subject\"] if pd.notnull(row[\"Processed_Subject\"]) else \"\"\n",
    "    body = row[\"Processed_Body\"] if pd.notnull(row[\"Processed_Body\"]) else \"\"\n",
    "    sender = row[\"From\"] if pd.notnull(row[\"From\"]) else \"\"\n",
    "\n",
    "    # Check for spam keywords in subject or body\n",
    "    keyword_match = bool(spam_pattern.search(subject)) or bool(spam_pattern.search(body))\n",
    "\n",
    "    # Check if sender is outside Enron (not ending in @enron.com)\n",
    "    domain_check = not sender.endswith(\"@enron.com\") if sender else False\n",
    "\n",
    "    return 1 if keyword_match or domain_check else 0\n",
    "\n",
    "# Apply function\n",
    "df[\"Spam_Label\"] = df.apply(is_spam, axis=1)\n",
    "\n",
    "# Display spam vs non-spam counts\n",
    "print(df[\"Spam_Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'To' column as it's not useful\n",
    "df = df.drop(columns=[\"To\"], errors=\"ignore\")\n",
    "\n",
    "# Remove rows with missing values in key fields\n",
    "df = df.dropna(subset=[\"From\", \"Processed_Subject\", \"Processed_Body\"])\n",
    "\n",
    "# Feature Engineering: Word Count in Subject & Body\n",
    "df[\"Word_Count_Subject\"] = df[\"Processed_Subject\"].apply(lambda x: len(x.split()))\n",
    "df[\"Word_Count_Body\"] = df[\"Processed_Body\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Display updated dataset\n",
    "print(df[[\"Processed_Subject\", \"Processed_Body\", \"Word_Count_Subject\", \"Word_Count_Body\", \"Spam_Label\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialise TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=500, ngram_range=(1,2))\n",
    "\n",
    "# Transform Subject & Body separately\n",
    "tfidf_subject = vectorizer.fit_transform(df[\"Processed_Subject\"])\n",
    "tfidf_body = vectorizer.fit_transform(df[\"Processed_Body\"])\n",
    "\n",
    "# Merge TF-IDF features into a single sparse matrix\n",
    "X = hstack([tfidf_subject, tfidf_body])  # Sparse format to reduce memory usage\n",
    "y = df[\"Spam_Label\"].values  # Convert labels to NumPy array\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))  # Should be <class 'scipy.sparse._csr.csr_matrix'>\n",
    "print(X_train.shape)  # Should match expected training size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"Spam_Label\", \"From\", \"file\", \"message\"], errors=\"ignore\")\n",
    "y = df[\"Spam_Label\"]\n",
    "\n",
    "# Split dataset (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"y_train type: {type(y_train)}\")\n",
    "print(f\"Unique values in y_train: {np.unique(y_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))  \n",
    "print(X_train.shape)  # Should match expected training size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Merge subject and body TF-IDF vectors\n",
    "X = hstack([tfidf_subject, tfidf_body])  # Ensure it's a sparse matrix\n",
    "y = df[\"Spam_Label\"].values  # Convert labels to NumPy array\n",
    "\n",
    "# Split Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train Naïve Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)  \n",
    "\n",
    "# Predict on test data\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Naïve Bayes Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\",\n",
    "            xticklabels=[\"Not Spam\", \"Spam\"], yticklabels=[\"Not Spam\", \"Spam\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Naïve Bayes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train Naïve Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)  \n",
    "\n",
    "# Predict on test data\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Naïve Bayes Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\",\n",
    "            xticklabels=[\"Not Spam\", \"Spam\"], yticklabels=[\"Not Spam\", \"Spam\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Naïve Bayes\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
